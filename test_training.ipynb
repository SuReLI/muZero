{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from src.training import MuModel\n",
    "from src.training.losses import Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt types: <class 'torch.optim.adam.Adam'>, <class 'torch.optim.adam.Adam'>, <class 'torch.optim.adam.Adam'>\n",
      "tensor([1])\n"
     ]
    }
   ],
   "source": [
    "# Let's suppose the cartpole env, which has a 4-dim observation space and a 2-dim action space\n",
    "\n",
    "K = 3\n",
    "N = 5\n",
    "action_dim = 2\n",
    "observation_dim = 4\n",
    "\n",
    "lr = nn.MSELoss()\n",
    "lv = nn.MSELoss()\n",
    "lp = nn.CrossEntropyLoss()\n",
    "\n",
    "criterion = Loss(lr=lr,lv=lv,lp=lp)\n",
    "\n",
    "model = MuModel(\n",
    "    observation_dim=observation_dim,\n",
    "    action_dim=action_dim,\n",
    "    state_dim=128,\n",
    "    N=5,\n",
    "    K=3,\n",
    "    criterion=criterion\n",
    ")\n",
    "\n",
    "# some random inputs\n",
    "batch_size = 1\n",
    "\n",
    "horizon_target = torch.randint(1,K+1,size=(batch_size,))\n",
    "# horizon_target = torch.tensor([3])\n",
    "\n",
    "batch = (\n",
    "    torch.rand(batch_size, N, observation_dim), # observations\n",
    "    torch.rand(batch_size, K, action_dim),      # target policies\n",
    "    torch.rand(batch_size, K, action_dim),      # target actions\n",
    "    torch.rand(batch_size, K),                  # target rewards\n",
    "    torch.rand(batch_size, K),                  # target returns\n",
    "    horizon_target\n",
    ")\n",
    "\n",
    "print(horizon_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opt types: <class 'torch.Tensor'>, <class 'torch.optim.adam.Adam'>, <class 'torch.optim.adam.Adam'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'zero_grad'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Personal/indepth-rl/muZero/src/training/mu_model.py:186\u001b[0m, in \u001b[0;36mMuModel.training_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    182\u001b[0m     (observations, target_policies, target_actions, \n\u001b[1;32m    183\u001b[0m      target_rewards, target_returns, target_horizon) \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m--> 186\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_models_one_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_policies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_actions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_rewards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_returns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_horizon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt_g\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt_f\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#verbose=True\u001b[39;49;00m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/Documents/Personal/indepth-rl/muZero/src/training/mu_model.py:102\u001b[0m, in \u001b[0;36mMuModel._train_models_one_step\u001b[0;34m(observations, target_policies, target_actions, target_rewards, target_returns, target_horizon, optimizer_h, optimizer_g, optimizer_f, criterion, h, g, f, horizon, verbose)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Set gradients to zero\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpt types: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(optimizer_h)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(optimizer_g)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(optimizer_f)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 102\u001b[0m \u001b[43moptimizer_h\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m()\n\u001b[1;32m    103\u001b[0m optimizer_g\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    104\u001b[0m optimizer_f\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'zero_grad'"
     ]
    }
   ],
   "source": [
    "# forward pass\n",
    "model.training_step(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.3328\n"
     ]
    }
   ],
   "source": [
    "model.validation_step(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Training Loss: 2.2071\n",
      "Training Loss: 2.2081\n",
      "Training Loss: 2.2082\n",
      "Training Loss: 2.2077\n",
      "Training Loss: 2.2078\n",
      "Validation Loss: 2.2073\n",
      "Epoch 1\n",
      "Training Loss: 2.2073\n",
      "Training Loss: 2.2079\n",
      "Training Loss: 2.2075\n",
      "Training Loss: 2.2075\n",
      "Training Loss: 2.2083\n",
      "Validation Loss: 2.2069\n",
      "Epoch 2\n",
      "Training Loss: 2.2069\n",
      "Training Loss: 2.2079\n",
      "Training Loss: 2.2076\n",
      "Training Loss: 2.2074\n",
      "Training Loss: 2.2079\n",
      "Validation Loss: 2.2077\n",
      "Epoch 3\n",
      "Training Loss: 2.2077\n",
      "Training Loss: 2.2074\n",
      "Training Loss: 2.2084\n",
      "Training Loss: 2.2070\n",
      "Training Loss: 2.2079\n",
      "Validation Loss: 2.2070\n",
      "Epoch 4\n",
      "Training Loss: 2.2070\n",
      "Training Loss: 2.2079\n",
      "Training Loss: 2.2074\n",
      "Training Loss: 2.2073\n",
      "Training Loss: 2.2083\n",
      "Validation Loss: 2.2078\n"
     ]
    }
   ],
   "source": [
    "n_step_grad = 5\n",
    "n_epoch = 5\n",
    "\n",
    "for k in range(n_epoch):\n",
    "    print(f\"Epoch {k}\")\n",
    "    for _ in range(n_step_grad):\n",
    "        model.training_step(batch)\n",
    "    model.validation_step(batch)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gengar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
